{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hallucination Detection\n",
                "\n",
                "<img src=\"https://live.staticflickr.com/65535/54208132682_73767c3560_b.jpg\" alt=\"Embedded Photo\" width=\"500\">\n",
                "\n",
                "*Image generated using DALL-E model.*\n",
                "\n",
                "## Introduction\n",
                "\n",
                "Language models help us with daily tasks such as correcting texts, writing code, or answering questions. \n",
                "They are also increasingly used in fields like medicine and education.\n",
                "\n",
                "However, how can we know if the answers generated by them are correct? Language models do not always have full knowledge of a given topic, yet they can formulate answers that sound plausible but are actually misleading. Such incorrect answers are called hallucinations.\n",
                "\n",
                "## Task\n",
                "\n",
                "In this task, you will face the challenge of detecting hallucinations in factual answers generated by large language models (LLMs).\n",
                "You will analyze a dataset to help assess whether the answers generated by the language model are indeed correct or contain hallucinations.\n",
                "\n",
                "Each example in the dataset contains:\n",
                "\n",
                "- **Question** e.g. \"What is the main responsibility of the US Department of Defense?\"\n",
                "- **Language model answer** e.g. \"The main responsibility is national defense.\"\n",
                "- **Tokens** associated with answer generation.\n",
                "- **Four alternative answers** generated by the same model with higher temperature.\n",
                "- **Alternative answer tokens** generated by the same model with higher temperature.\n",
                "- **Alternative answer probabilities** generated by the same model with higher temperature.\n",
                "- **Label (`is_correct`)** indicating whether the main answer is correct according to a trusted source.\n",
                "\n",
                "\n",
                "Example:\n",
                "```json\n",
                "[\n",
                "    {\n",
                "        \"question_id\": 34,\n",
                "        \"question\": \"What is the name of the low-cost carrier that operates as a wholly owned subsidiary of Singapore Airlines?\",\n",
                "        \"answer\": \"Scoot is the low-cost carrier that operates as a wholly owned subsidiary of Singapore Airlines.\",\n",
                "        \"tokens\": [\" Sco\", \"ot\", \" is\", ..., \" Airlines\", \".\", \"\\n\"],\n",
                "        \"supporting_answers\": [\n",
                "            \"As a wholly owned subsidiary of Singapore Airlines, <answer> Scoot </answer> stands as a low-cost carrier that revolutionized air travel in the region.\",\n",
                "            \"Scoot, a subsidiary of <answer> Singapore Airlines </answer> , is the low-cost carrier that operates under the same brand.\",\n",
                "            \"<answer> Scoot </answer> is the low-cost carrier that operates as a wholly owned subsidiary of Singapore Airlines.\",\n",
                "            \"Singapore Airlines operates a low-cost subsidiary named <answer> Scoot </answer> , offering affordable and efficient air travel options to passengers.\"\n",
                "        ],\n",
                "        \"supporting_tokens\": [\n",
                "            [\" As\", \" a\", ..., \".\", \"<answer>\"],\n",
                "            [\" Sco\", \"ot\", ..., \" brand\", \".\", \"\\n\"],\n",
                "            [\"<answer>\", \" Sco\", ..., \".\", \"\\n\"],\n",
                "            [\" Singapore\", \" Airlines\", ..., \".\", \"\\n\"]\n",
                "        ],\n",
                "        \"supporting_probabilities\": [\n",
                "            [0.0029233775567263365, 0.8621460795402527, ..., 0.018515007570385933],\n",
                "            [0.42073577642440796, 0.9999748468399048, ..., 0.9166142344474792],\n",
                "            [0.3258324861526489, 0.9969879984855652, ..., 0.921079695224762],\n",
                "            [0.11142394691705704, 0.960810661315918, ..., 0.9557166695594788]\n",
                "        ],\n",
                "        \"is_correct\": true\n",
                "    },\n",
                "    .\n",
                "    .\n",
                "    .\n",
                "]\n",
                "```\n",
                "\n",
                "### Data\n",
                "The data available to you in this task are:\n",
                "\n",
                "* `train.json` - dataset containing 2967 questions and answers.\n",
                "* `valid.json` - 990 additional questions.\n",
                "\n",
                "\n",
                "### Evaluation Criteria\n",
                "\n",
                "ROC AUC (*Receiver Operating Characteristic Area Under Curve*) is a measure of binary classifier quality. It shows the model's ability to distinguish between two classes - here hallucination (false) and correct answer (true).\n",
                "\n",
                "- **ROC (Receiver Operating Characteristic)**: A plot showing the relationship between *True Positive Rate* (sensitivity) and *False Positive Rate* (1-specificity) at different decision thresholds.\n",
                "- **AUC (Area Under Curve)**: The area under the ROC curve, which takes values from 0 to 1:\n",
                "  - **1.0**: Perfect model.\n",
                "  - **0.5**: Random model (no ability to distinguish classes).\n",
                "\n",
                "The higher the AUC value, the better the model performs at classification.\n",
                "\n",
                "For this task, you can score between 0 and 100 points. The score will be scaled linearly depending on the ROC AUC value:\n",
                "\n",
                "- **ROC AUC ≤ 0.7**: 0 points.\n",
                "- **ROC AUC ≥ 0.82**: 100 points.\n",
                "- **Values between 0.7 and 0.82**: scaled linearly.\n",
                "\n",
                "Score Formula:  \n",
                "$$\n",
                "\\text{Points} = \n",
                "\\begin{cases} \n",
                "0 & \\text{for } \\text{ROC AUC} \\leq 0.7 \\\\\n",
                "100 \\times \\frac{\\text{ROC AUC} - 0.7}{0.82 - 0.7} & \\text{for } 0.7 < \\text{ROC AUC} < 0.82 \\\\\n",
                "100 & \\text{for } \\text{ROC AUC} \\geq 0.82\n",
                "\\end{cases}\n",
                "$$\n",
                "\n",
                "\n",
                "## Constraints\n",
                "* Your solution will be tested on the Competition Platform without internet access and in a GPU-free environment.\n",
                "* The evaluation of your final solution on the Competition Platform cannot take longer than 5 minutes without a GPU.\n",
                "* List of allowed libraries: `xgboost`, `scikit-learn`, `numpy`, `pandas`, `matplotlib`.\n",
                "\n",
                "\n",
                "## Submission Files\n",
                "This notebook supplemented with your solution (see `predict_hallucinations` function).\n",
                "\n",
                "## Evaluation\n",
                "Remember that during checking, the `FINAL_EVALUATION_MODE` flag will be set to `True`.\n",
                "\n",
                "For this task, you can score between 0 and 100 points. The score will be calculated on (secret) test set on the Competition Platform based on the formula mentioned above, rounded to an integer. If your solution does not meet the above criteria or does not execute correctly, you will receive 0 points for the task.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Starter Code\n",
                "In this section, we initialize the environment by importing necessary libraries and functions. The prepared code will facilitate your efficient operation on data and building the proper solution."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "FINAL_EVALUATION_MODE = False\n",
                "import os\n",
                "import json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import sklearn as sk\n",
                "from sklearn.metrics import roc_auc_score\n",
                "import xgboost as xgb\n",
                "import shutil\n",
                "\n",
                "def download_data(train=(\"1TGEDaxw4GKfSq0fpqSk0wRpUSc8GgZN0\", \"train.json\"),\n",
                "                  valid=(\"1qrr7bZk6Uct8DeC-V8Bc1qD5su56ryFd\", \"valid.json\")):\n",
                "    \"\"\"Downloads dataset from Google Drive and saves it in 'data' folder.\"\"\"\n",
                "    import gdown\n",
                "    \n",
                "    # Create or reset 'data' folder\n",
                "    if not os.path.exists('data'):\n",
                "        os.makedirs('data')\n",
                "    else:\n",
                "        shutil.rmtree('data')\n",
                "        os.makedirs('data')\n",
                "\n",
                "    GDRIVE_DATA = [train, valid]\n",
                "    \n",
                "    for file_id, file_name in GDRIVE_DATA:        \n",
                "        # Download file from Google Drive and save it in 'data' folder\n",
                "        url = f'https://drive.google.com/uc?id={file_id}'\n",
                "        output = f'data/{file_name}'\n",
                "        gdown.download(url, output, quiet=False)\n",
                "        \n",
                "        print(f\"Downloaded: {file_name}\")\n",
                "\n",
                "# Download data only if not in FINAL_EVALUATION_MODE\n",
                "if not FINAL_EVALUATION_MODE:\n",
                "    download_data()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading Data\n",
                "Using the code below, data will be loaded and properly prepared."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_data(folder='data'):\n",
                "    # Load data from files\n",
                "    train_path = os.path.join(folder, 'train.json')\n",
                "    valid_path = os.path.join(folder, 'valid.json')\n",
                "    \n",
                "    with open(train_path, 'r') as f:\n",
                "        train = json.load(f)\n",
                "    with open(valid_path, 'r') as f:\n",
                "        valid = json.load(f)\n",
                "\n",
                "    return train, valid\n",
                "\n",
                "train, valid = load_data(\"data\")\n",
                "\n",
                "# print(json.dumps(train[0], indent=2))\n",
                "\n",
                "print(f\"\\nAll training examples: {len(train)}\")\n",
                "print(f\"All validation examples: {len(valid)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Code with Evaluation Criteria\n",
                "\n",
                "Code similar to the one below will be used to evaluate the solution on the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_score(roc_auc: float) -> float:\n",
                "    \"\"\"\n",
                "    Compute score based on ROC AUC.\n",
                "\n",
                "    :param roc_auc: Float value in range [0.0, 1.0]\n",
                "    :return: Score consistent with the specified function\n",
                "    \"\"\"\n",
                "    if roc_auc <= 0.7:\n",
                "        return 0\n",
                "    elif 0.7 < roc_auc < 0.82:\n",
                "        return int(round(100 * (roc_auc - 0.7) / (0.82 - 0.7)))\n",
                "    else:\n",
                "        return 100\n",
                "\n",
                "\n",
                "def evaluate_algorithm(dataset, algorithm, verbose=False):\n",
                "    \"\"\"\n",
                "    Evaluate algorithm for hallucination detection on the given dataset.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    dataset : list\n",
                "        Annotated dataset, where each element is a dictionary containing the 'is_correct' key.\n",
                "    algorithm : callable\n",
                "        Function that takes a single sample (dictionary) and returns the probability of hallucination.\n",
                "    verbose : bool\n",
                "        If True, prints additional information for each sample and summary.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    roc_auc : float\n",
                "        ROC AUC score for the predictions.\n",
                "    \"\"\"\n",
                "    predicted_ys = [] # List storing predicted hallucination probabilities\n",
                "\n",
                "    for i, entry in enumerate(dataset):\n",
                "        # Create a copy of the sample and remove the label to get unlabeled input\n",
                "        sample_unlabeled = dict(entry)\n",
                "        sample_unlabeled.pop('is_correct', None)\n",
                "\n",
                "        try:\n",
                "            # Prediction of probability for a single sample\n",
                "            pred_prob = algorithm(sample_unlabeled)\n",
                "            predicted_ys.append(pred_prob)\n",
                "\n",
                "        except Exception as e:\n",
                "            # If an error occurs, default probability is set to 0.5\n",
                "            predicted_ys.append(0.5)\n",
                "            if verbose:\n",
                "                print(f\"Sample {i} => Error: {e}\")\n",
                "\n",
                "    predicted_ys = np.array(predicted_ys, dtype=np.float32)\n",
                "    ys = []\n",
                "    for entry in dataset:\n",
                "        ys.append(1 if entry.get('is_correct') else 0)\n",
                "    ys = np.array(ys, dtype=np.int32)\n",
                "    \n",
                "    # Calculate ROC AUC metric\n",
                "    roc_auc = roc_auc_score(ys, predicted_ys)\n",
                "\n",
                "    # Calculate final score based on ROC AUC\n",
                "    points = compute_score(roc_auc)\n",
                "\n",
                "    if verbose:\n",
                "        print(f\"\\nNumber of samples: {len(dataset)}\")\n",
                "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
                "        print(f\"Score: {points}\")\n",
                "\n",
                "    return points"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Your Solution\n",
                "In this section, you should place your solution. Make changes only here!\n",
                "\n",
                "## Technical Breakdown of the Solution\n",
                "\n",
                "The implemented approach relies on **advanced feature engineering** and a **Gradient Boosting (XGBoost)** classifier. Below is a detailed breakdown of the components:\n",
                "\n",
                "### 1. Feature Engineering (`extract_features`)\n",
                "To distinguish between correct answers and hallucinations, we extract several types of features from the provided samples:\n",
                "\n",
                "- **Probability Statistics**: We calculate the mean, standard deviation, skewness, and range of the `supporting_probabilities`. Hallucinations often show lower mean confidence and higher variance across alternative generations.\n",
                "- **Semantic Consistency**: We check how often the key phrase or word from the main answer appears in the `supporting_answers`. A high consistency ratio suggests a stable (and likely correct) fact.\n",
                "- **Agreement Analysis**: We extract the 'tagged' answers from all versions and calculate an `agreement_ratio`. If the model consistently produces the same answer across different runs (even with higher temperature), it increases the probability of truth.\n",
                "- **Lexical Overlap**: We measure the set overlap between the question and the answer to capture if the model is simply repeating the prompt or providing new (potentially hallucinated) information.\n",
                "- **Entropy & Specificity**: We calculate the shannon entropy of the answer words and a specificity metric. Vague or repetitive answers often correlate with specific model behaviors related to hallucination.\n",
                "\n",
                "### 2. Model Configuration\n",
                "We use an `XGBClassifier` with the following optimizations:\n",
                "- **AUC Optimization**: The model is specifically tuned to maximize ROC AUC, as required by the task constraints.\n",
                "- **Regularization**: `reg_alpha` and `reg_lambda` are used to prevent overfitting on the limited training data.\n",
                "- **Early Stopping**: To ensure optimal generalization and prevent training for too many rounds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_features(sample):\n",
                "    question = sample['question']\n",
                "    answer = sample['answer']\n",
                "    tokens = sample['tokens']\n",
                "    supporting_answers = sample['supporting_answers']\n",
                "    supporting_probabilities = sample['supporting_probabilities']\n",
                "    supporting_tokens = sample['supporting_tokens']\n",
                "    \n",
                "    token_count = len(tokens)\n",
                "    answer_words = len(answer.split())\n",
                "    answer_words_list = answer.lower().split()\n",
                "    question_words = set(question.lower().split())\n",
                "    \n",
                "    all_probs = [p for sublist in supporting_probabilities for p in sublist]\n",
                "    prob_mean = np.mean(all_probs) if all_probs else 0\n",
                "    prob_std = np.std(all_probs) if all_probs else 0\n",
                "    prob_min = np.min(all_probs) if all_probs else 0\n",
                "    prob_max = np.max(all_probs) if all_probs else 0\n",
                "    prob_range = prob_max - prob_min if all_probs else 0\n",
                "    \n",
                "    prob_skew = pd.Series(all_probs).skew() if all_probs else 0\n",
                "    \n",
                "    key_phrase = ' '.join(answer_words_list[-3:]) if len(answer_words_list) >= 3 else answer.lower()\n",
                "    key_word = answer_words_list[-1]\n",
                "    consistency_count_phrase = sum(1 for sa in supporting_answers if key_phrase in sa.lower().replace('<answer>', '').replace('</answer>', ''))\n",
                "    consistency_count_word = sum(1 for sa in supporting_answers if key_word in sa.lower().replace('<answer>', '').replace('</answer>', ''))\n",
                "    consistency_ratio_phrase = consistency_count_phrase / len(supporting_answers)\n",
                "    consistency_ratio_word = consistency_count_word / len(supporting_answers)\n",
                "    \n",
                "    answer_words_set = set(answer_words_list)\n",
                "    overlap = len(question_words & answer_words_set) / max(len(question_words), 1)\n",
                "    \n",
                "    prob_vars = [np.var(sublist) for sublist in supporting_probabilities if sublist]\n",
                "    prob_var_mean = np.mean(prob_vars) if prob_vars else 0\n",
                "    \n",
                "    question_len = len(question.split())\n",
                "    rel_length = answer_words / max(question_len, 1)\n",
                "    \n",
                "    tagged_answers = [sa[sa.find('<answer>')+8:sa.find('</answer>')].strip().lower() \n",
                "                     for sa in supporting_answers if '<answer>' in sa and '</answer>' in sa]\n",
                "    most_common_answer = max(set(tagged_answers), key=tagged_answers.count, default='')\n",
                "    agreement_ratio = tagged_answers.count(most_common_answer) / len(tagged_answers) if tagged_answers else 0\n",
                "    answer_matches_most_common = 1 if most_common_answer and most_common_answer in answer.lower() else 0\n",
                "    \n",
                "    prob_conf_dev = abs(prob_mean - 0.5)\n",
                "    high_conf_ratio = sum(1 for p in all_probs if p > 0.95) / len(all_probs) if all_probs else 0  # Tighter threshold\n",
                "    \n",
                "    specificity = len(answer_words_set) / max(answer_words, 1)\n",
                "    \n",
                "    supp_tokens_flat = ' '.join([t.lower() for sublist in supporting_tokens for t in sublist]).strip()\n",
                "    token_mismatch = sum(1 for t in tokens if t.strip().lower() not in supp_tokens_flat) / max(token_count, 1)\n",
                "    \n",
                "    word_freq = pd.Series(answer_words_list).value_counts(normalize=True)\n",
                "    answer_entropy = -sum(p * np.log2(p) for p in word_freq if p > 0)\n",
                "    \n",
                "    supp_prob_means = [np.mean(sublist) for sublist in supporting_probabilities if sublist]\n",
                "    prob_discrepancy = np.std(supp_prob_means) if supp_prob_means else 0\n",
                "    \n",
                "    key_question_words = [w for w in question.lower().split() if len(w) > 3 and w not in {'what', 'where', 'when', 'who', 'how'}]\n",
                "    key_question_presence = sum(1 for w in key_question_words if w in answer.lower()) / max(len(key_question_words), 1)\n",
                "    \n",
                "    supp_lengths = [len(sa.replace('<answer>', '').replace('</answer>', '').split()) for sa in supporting_answers]\n",
                "    supp_len_std = np.std(supp_lengths) if supp_lengths else 0\n",
                "    supp_len_mean = np.mean(supp_lengths) if supp_lengths else 0\n",
                "    answer_len_deviation = abs(answer_words - supp_len_mean) / max(supp_len_mean, 1)\n",
                "    \n",
                "    return [\n",
                "        token_count, answer_words, prob_mean, prob_std, prob_min, prob_max, prob_range, prob_skew,\n",
                "        consistency_ratio_phrase, consistency_ratio_word, overlap, prob_var_mean, rel_length,\n",
                "        agreement_ratio, answer_matches_most_common, prob_conf_dev, high_conf_ratio, specificity,\n",
                "        token_mismatch, answer_entropy, prob_discrepancy, key_question_presence, supp_len_std,\n",
                "        answer_len_deviation\n",
                "    ]\n",
                "\n",
                "def prepare_data(dataset):\n",
                "    X = []\n",
                "    y = []\n",
                "    for sample in dataset:\n",
                "        features = extract_features(sample)\n",
                "        X.append(features)\n",
                "        y.append(1 if sample['is_correct'] else 0)\n",
                "    return np.array(X), np.array(y)\n",
                "\n",
                "combined_data = train + valid\n",
                "train_X, train_y = prepare_data(combined_data)\n",
                "\n",
                "model = xgb.XGBClassifier(\n",
                "    n_estimators=500,\n",
                "    max_depth=4,\n",
                "    learning_rate=0.015,\n",
                "    subsample=0.7,\n",
                "    colsample_bytree=0.7,\n",
                "    reg_alpha=0.5,\n",
                "    reg_lambda=1.5,\n",
                "    random_state=42,\n",
                "    eval_metric='auc',\n",
                "    early_stopping_rounds=20\n",
                ")\n",
                "\n",
                "valid_X, valid_y = prepare_data(valid)\n",
                "model.fit(train_X, train_y, eval_set=[(valid_X, valid_y)], verbose=False)\n",
                "\n",
                "def predict_hallucinations(sample):\n",
                "    features = extract_features(sample)\n",
                "    features_array = np.array([features])\n",
                "    prob = model.predict_proba(features_array)[0][1]\n",
                "    return prob"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluation\n",
                "\n",
                "Running the cell below will allow you to check how many points your solution would score on validation data. Before sending, ensure that the entire notebook executes from start to finish without errors and without user intervention after selecting \"Run All\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Liczba próbek: 990\n",
                        "ROC AUC: 0.8977\n",
                        "Wynik punktowy: 100\n"
                    ]
                }
            ],
            "source": [
                "if not FINAL_EVALUATION_MODE:\n",
                "    roc_auc = evaluate_algorithm(valid, predict_hallucinations, verbose=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "During checking, the model will be saved as `your_model.pkl` and evaluated on the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if FINAL_EVALUATION_MODE:      \n",
                "    import cloudpickle\n",
                "      \n",
                "    OUTPUT_PATH = \"file_output\"\n",
                "    FUNCTION_FILENAME = \"your_model.pkl\"\n",
                "    FUNCTION_OUTPUT_PATH = os.path.join(OUTPUT_PATH, FUNCTION_FILENAME)\n",
                "\n",
                "    if not os.path.exists(OUTPUT_PATH):\n",
                "        os.makedirs(OUTPUT_PATH)\n",
                "\n",
                "    with open(FUNCTION_OUTPUT_PATH, \"wb\") as f:\n",
                "        cloudpickle.dump(predict_hallucinations, f)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}